* Spark resiliency

 Driver: the driver auto restart if it exist with non-zero code in cluster mode
 
 Master: scheduler, cluster manager, we can setup multi master using zk. 
 
 Worker process:
 
 Executor:
 
 Receiver:
 

* Failure

 Driver failure: back-pressure build up...drop message
 
 Receiver failure: depends on Kafka WAL...
 

* exactly once for Spark streaming:

 Kafka, offset, WAL, direct API
 
 At-least once deliver: WAL写成功, but写offset时挂了, 此时recovery时会重复消费数据.
 
 Direct API: Don't write ahead log due to bad performance, executors consume the data from Kafka directly(not via WAL and Receiver)

* reliable message processing

 reliable source(Kafka)
 
 reliable receiver ack using update offset to Kafka(guarante input side)
 
 receiver replicate data to workers, and update block IDs to driver streaming context;  (WAL)
 
 Driver persist the context data like checkpoints/metadata to HDFS or S3 for reliability

* ML 

 spark.mllib for 1.0, spark.ml is for ML pipeline that is from spark ML 1.2
 
 Word2Vec: words to K vector, and then compute the euclidean distance or cos to compare the similiarity. Word2Vec 采用的是 Distributed representation 的词向量表示方式，这种表达方式不仅可以有效控制词向量的维度，避免维数灾难 (相对于 one-hot representation)，而且可以保证意思相近的词在向量空间中的距离较近。CBOW (Continuous Bag of Words, 连续词袋模型) 和 Skip-Gram, CBOW 是根据语境预测目标单词，Skip-Gram 根据当前单词预测语境。Spark 的实现采用的是 Skip-Gram 模型; 在 Skip-Gram 模型里，每个单词都关联两个向量，分别表示词向量和上下文向量。也正是因为如此，Word2Vec 较之传统的 LDA(Latent Dirichlet Allocation) 过程，可以表达更加丰富和准确的语义信息。

 HashingTF & LogisticRegression
 
 HashingTF: extract features and pass the vectors to classfication estimator.
 
 LogisticRegression: classification estimator.  model.predict(...)
 
 Regression: iterative computation, math.
 
 Linear regression
 
 Decision tree
 
 Multilayer perception classfier 多层感知分类器, input ,hidden layer, output layer, 前馈型神经网络,
 
 Naive Bayes
 
 Random forest

 CF: recommendation
 
 Clustering: K-means (unsupervised learning)一般是用来对数据对象按照其特征属性进行分组，经常被应用在客户分群，欺诈检测，图像分析等领域; 选中多个点计算其他点到他们的欧几里得距离euclidean distance,迭代k次直到稳定.  result is the cluster id for given object

 Make H(loss function) as small as possible, so have to regress many rounds of fitting using min square 最小二乘数algorithm.
 
 H=1/2 sum ( h(x)- y) ^ 2
 
 why Min square: 概率论  (or using gradient decsent梯度下降)
 











