* Spark resiliency

 Driver: the driver auto restart if it exist with non-zero code in cluster mode
 
 Master: scheduler, cluster manager, we can setup multi master using zk. 
 
 Worker process:
 
 Executor:
 
 Receiver:
 

* Failure

 Driver failure: back-pressure build up...drop message
 
 Receiver failure: depends on Kafka WAL...
 

* exactly once for Spark streaming:

 Kafka, offset, WAL, direct API
 
 At-least once deliver: WAL写成功, but写offset时挂了, 此时recovery时会重复消费数据.
 
 Direct API: Don't write ahead log due to bad performance, executors consume the data from Kafka directly(not via WAL and Receiver)

* reliable message processing

 reliable source(Kafka)
 
 reliable receiver ack using update offset to Kafka(guarante input side)
 
 receiver replicate data to workers, and update block IDs to driver streaming context;  (WAL)
 
 Driver persist the context data like checkpoints/metadata to HDFS or S3 for reliability

* ML 

 spark.mllib for 1.0, spark.ml is for ML pipeline that is from spark ML 1.2

 HashingTF & LogisticRegression
 
 HashingTF: extract features and pass the vectors to classfication estimator.
 
 LogisticRegression: classification estimator.  model.predict(...)
 
 Regression: iterative computation, math.
 
 Linear regression
 
 Decision tree
 
 Multilayer perception classfier 多层感知分类器, input ,hidden layer, output layer, 前馈型神经网络,
 
 Naive Bayes
 
 Random forest

 CF: recommendation
 
 Clustering: K-means (unsupervised learning)一般是用来对数据对象按照其特征属性进行分组，经常被应用在客户分群，欺诈检测，图像分析等领域; 选中多个点计算其他点到他们的欧几里得距离euclidean distance,迭代k次直到稳定.  result is the cluster id for given object

 Make H(loss function) as small as possible, so have to regress many rounds of fitting using min square 最小二乘数algorithm.
 
 H=1/2 sum ( h(x)- y) ^ 2
 
 why Min square: 概率论  (or using gradient decsent梯度下降)
 











