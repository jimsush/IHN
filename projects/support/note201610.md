* Spark resiliency
Driver: the driver auto restart if it exist with non-zero code in cluster mode
Master: scheduler, cluster manager, we can setup multi master using zk.
Worker process:
Executor:
Receiver:

* Failure
Driver failure: back-pressure build up...drop message
Receiver failure: depends on Kafka WAL...

* exactly once for Spark streaming:
Kafka, offset, WAL, direct API
At-least once deliver: WAL写成功, but写offset时挂了, 此时recovery时会重复消费数据.
Direct API: Don't write ahead log due to bad performance, executors consume the data from Kafka directly(not via WAL and Receiver)

* reliable message processing
reliable source(Kafka)
reliable receiver ack using update offset to Kafka(guarante input side)
receiver replicate data to workers, and update block IDs to driver streaming context;  (WAL)
Driver persist the context data like checkpoints/metadata to HDFS or S3 for reliability

* ML 
HashingTF & LogisticRegression
HashingTF: extract features and pass the vectors to classfication estimator.
LogisticRegression: classification estimator.  model.predict(...)
Regression: iterative computation, math.
Linear regression
Decision tree
Multilayer perception classfier
Naive Bayes
Random forest

CF: recommendation
Clustering: K-means (non-supervised)

Make H(loss function) as small as possible, so have to regress many rounds of fitting using min square algrithm.
H=1/2 sum ( h(x)- y) ^ 2
why Min square: 概率论  (or using gradient decsent)











